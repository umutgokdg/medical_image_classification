{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "# Stratified train-test split ensuring balanced gender representation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and test datasets\n",
    "train_metadata = pd.read_csv('female_train.csv')\n",
    "test_metadata_female = pd.read_csv('female_test.csv')\n",
    "test_metadata_male = pd.read_csv('male_test.csv')\n",
    "\n",
    "image_dir = 'images/all_images/'\n",
    "image_files = set(os.listdir(image_dir))\n",
    "\n",
    "# Function to check if the image exists\n",
    "def image_exists(img_id):\n",
    "    return img_id in image_files\n",
    "\n",
    "# Filter the metadata to only include rows with existing images\n",
    "train_metadata['image_exists'] = train_metadata['img_id'].apply(image_exists)\n",
    "train_metadata_filtered = train_metadata[train_metadata['image_exists']]\n",
    "train_metadata_filtered = train_metadata_filtered.drop(columns=['image_exists'])\n",
    "\n",
    "test_metadata_female['image_exists'] = test_metadata_female['img_id'].apply(image_exists)\n",
    "test_metadata_filtered_female = test_metadata_female[test_metadata_female['image_exists']]\n",
    "test_metadata_filtered_female = test_metadata_filtered_female.drop(columns=['image_exists'])\n",
    "\n",
    "test_metadata_male['image_exists'] = test_metadata_male['img_id'].apply(image_exists)\n",
    "test_metadata_filtered_male = test_metadata_male[test_metadata_male['image_exists']]\n",
    "test_metadata_filtered_male = test_metadata_filtered_male.drop(columns=['image_exists'])\n",
    "\n",
    "# Map diagnostic to binary cancer labels\n",
    "diagnostic_map = {'BCC': 1, 'MEL': 1, 'SCC': 1, 'ACK': 0, 'NEV': 0, 'SEK': 0}\n",
    "train_metadata_filtered['diagnostic'] = train_metadata_filtered['diagnostic'].map(diagnostic_map)\n",
    "test_metadata_filtered_female['diagnostic'] = test_metadata_filtered_female['diagnostic'].map(diagnostic_map)\n",
    "test_metadata_filtered_male['diagnostic'] = test_metadata_filtered_male['diagnostic'].map(diagnostic_map)\n",
    "\n",
    "# Drop rows with NaN values in the 'diagnostic' column\n",
    "train_metadata_filtered = train_metadata_filtered.dropna(subset=['diagnostic'])\n",
    "test_metadata_filtered_female = test_metadata_filtered_female.dropna(subset=['diagnostic'])\n",
    "test_metadata_filtered_male = test_metadata_filtered_male.dropna(subset=['diagnostic'])\n",
    "\n",
    "# Create image paths\n",
    "train_metadata_filtered['image_path'] = image_dir + train_metadata_filtered['img_id']\n",
    "test_metadata_filtered_female['image_path'] = image_dir + test_metadata_filtered_female['img_id']\n",
    "test_metadata_filtered_male['image_path'] = image_dir + test_metadata_filtered_male['img_id']\n",
    "\n",
    "# Convert categorical features to numerical codes\n",
    "categorical_cols = ['smoke', 'drink', 'skin_cancer_history', 'cancer_history', 'has_piped_water', 'has_sewage_system', 'region', 'itch', 'grew', 'hurt', 'changed', 'bleed', 'elevation']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train_metadata_filtered[col] = train_metadata_filtered[col].astype('category').cat.codes\n",
    "    test_metadata_filtered_female[col] = test_metadata_filtered_female[col].astype('category').cat.codes\n",
    "    test_metadata_filtered_male[col] = test_metadata_filtered_male[col].astype('category').cat.codes\n",
    "\n",
    "train_df, val_df = train_test_split(train_metadata_filtered, test_size=0.2, stratify=train_metadata_filtered['gender'], random_state=42)\n",
    "\n",
    "# Separate features and target\n",
    "def prepare_data(metadata):\n",
    "    X_tabular = metadata[['age', 'smoke', 'drink', 'skin_cancer_history', 'cancer_history', 'has_piped_water', 'has_sewage_system', 'region', 'itch', 'grew', 'hurt', 'changed', 'bleed', 'elevation']].values\n",
    "    y = metadata['diagnostic'].values\n",
    "    image_paths = metadata['image_path'].values\n",
    "    return X_tabular, y, image_paths\n",
    "\n",
    "X_train_tabular, y_train, train_image_paths = prepare_data(train_df)\n",
    "X_val_tabular, y_val, val_image_paths = prepare_data(val_df)\n",
    "X_test_tabular_female, y_test_female, test_image_paths_female = prepare_data(test_metadata_filtered_female)\n",
    "X_test_tabular_male, y_test_male, test_image_paths_male = prepare_data(test_metadata_filtered_male)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tabular_data, image_paths, labels, transform=None):\n",
    "        self.tabular_data = tabular_data\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        tabular_data = torch.tensor(self.tabular_data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return tabular_data, image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the image data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(X_train_tabular, train_image_paths, y_train, transform=transform)\n",
    "val_dataset = CustomDataset(X_val_tabular, val_image_paths, y_val, transform=transform)\n",
    "test_dataset_female = CustomDataset(X_test_tabular_female, test_image_paths_female, y_test_female, transform=transform)\n",
    "test_dataset_male = CustomDataset(X_test_tabular_male, test_image_paths_male, y_test_male, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_female = DataLoader(test_dataset_female, batch_size=32, shuffle=False)\n",
    "test_loader_male = DataLoader(test_dataset_male, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalNetwork, self).__init__()\n",
    "        \n",
    "        # ResNet18 model for image data\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final layer\n",
    "        \n",
    "        # Multi Layer Perceptron for tabular data\n",
    "        self.fc_tabular = nn.Sequential(\n",
    "            nn.Linear(14, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined layers\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 128),  # Adjusted for ResNet18 output size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, tabular_data, images):\n",
    "        img_features = self.resnet(images)\n",
    "        tabular_features = self.fc_tabular(tabular_data)\n",
    "        combined_features = torch.cat((img_features, tabular_features), dim=1)\n",
    "        output = self.fc_combined(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umutg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\umutg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6467\n",
      "Epoch 2/100, Loss: 0.5050\n",
      "Epoch 3/100, Loss: 0.4067\n",
      "Epoch 4/100, Loss: 0.3527\n",
      "Epoch 5/100, Loss: 0.2474\n",
      "Epoch 6/100, Loss: 0.1480\n",
      "Epoch 7/100, Loss: 0.3861\n",
      "Epoch 8/100, Loss: 0.2881\n",
      "Epoch 9/100, Loss: 0.2191\n",
      "Epoch 10/100, Loss: 0.1576\n",
      "Epoch 11/100, Loss: 0.0999\n",
      "Early stopping at epoch 11\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MultimodalNetwork().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100  # Set a high number of epochs\n",
    "early_stop_threshold = 0.03  # Minimum loss decrease threshold\n",
    "early_stop_patience = 5  # Number of epochs to wait for improvement\n",
    "\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for tabular_data, images, labels in train_loader:\n",
    "        tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tabular_data, images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * tabular_data.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if epoch_loss < best_loss - early_stop_threshold:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience or epoch_loss < 0.1:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Test Loss: 2.0654, Accuracy: 0.7816, AUC-ROC: 0.7979\n",
      "Female Confusion Matrix:\n",
      "[[28 19]\n",
      " [ 0 40]]\n",
      "Female Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.60      0.75        47\n",
      "         1.0       0.68      1.00      0.81        40\n",
      "\n",
      "    accuracy                           0.78        87\n",
      "   macro avg       0.84      0.80      0.78        87\n",
      "weighted avg       0.85      0.78      0.77        87\n",
      "\n",
      "Male Test Loss: 1.7525, Accuracy: 0.6207, AUC-ROC: 0.6184\n",
      "Male Confusion Matrix:\n",
      "[[18 25]\n",
      " [ 8 36]]\n",
      "Male Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.42      0.52        43\n",
      "         1.0       0.59      0.82      0.69        44\n",
      "\n",
      "    accuracy                           0.62        87\n",
      "   macro avg       0.64      0.62      0.60        87\n",
      "weighted avg       0.64      0.62      0.60        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tabular_data, images, labels in test_loader:\n",
    "            tabular_data, images, labels = tabular_data.to(device), images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(tabular_data, images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * tabular_data.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    auc_roc = roc_auc_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "\n",
    "    return test_loss, accuracy, conf_matrix, auc_roc, report\n",
    "\n",
    "# Evaluate on female test dataset\n",
    "test_loss_female, accuracy_female, conf_matrix_female, auc_roc_female, report_female = evaluate_model(test_loader_female, model, criterion)\n",
    "print(f'Female Test Loss: {test_loss_female:.4f}, Accuracy: {accuracy_female:.4f}, AUC-ROC: {auc_roc_female:.4f}')\n",
    "print('Female Confusion Matrix:')\n",
    "print(conf_matrix_female)\n",
    "print('Female Classification Report:')\n",
    "print(report_female)\n",
    "\n",
    "# Evaluate on male test dataset\n",
    "test_loss_male, accuracy_male, conf_matrix_male, auc_roc_male, report_male = evaluate_model(test_loader_male, model, criterion)\n",
    "print(f'Male Test Loss: {test_loss_male:.4f}, Accuracy: {accuracy_male:.4f}, AUC-ROC: {auc_roc_male:.4f}')\n",
    "print('Male Confusion Matrix:')\n",
    "print(conf_matrix_male)\n",
    "print('Male Classification Report:')\n",
    "print(report_male)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
